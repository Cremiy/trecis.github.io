Run,date,team,description,paper,code,nDCG@100,Info-Type F1 [Actionable],Info-Type F1 [All],Info-Type Accuracy,Priority F1 [Actionable],Priority F1 [All],Priority R [Actionable],Priority R [All] 
njit_augly,2021/07/19,njit,NJIT RoBERTa run with AugLy in place of synonym augmentation.,,,0.5407,0.2477,0.2977,0.8690,0.1662,0.1792,0.1421,0.2014
njit_label_prop,2021/07/01,njit,"Model uses GPT2 to generate tweets and label propagation to label them, using which we train a new transformer model.",,,0.5717,0.2072,0.2823,0.8841,0.1599,0.1730,0.1728,0.2099
ens,2021/06/29,ucd-cs,"The ensemble run of multi-task learning with Deberta and eda augmentation, i.e., ucdcs-run4 at 2021a (alignment issue fixed)",https://lill.is/site/pubs/Wang2021b.html,https://github.com/wangcongcong123/crisis-mtl,0.5907🏆,0.2579,0.3211🏆,0.8646,0.3052🏆,0.3125🏆,0.3250🏆,0.3416🏆
njit_semi_sup,2021/06/27,njit,"Model uses GPT2 to generate tweets and semi-supervision to label them, using which we train a new transformer model.",,,0.5729,0.2429,0.3036,0.8888🏆,0.1599,0.1730,0.1728,0.2082
njit_semi_sup,2021/07/06,njit,Semi-supervised generation pipeline with priority scores using highest average score of information type labels.,,,0.5584,0.2072,0.2823,0.8841,0.2785,0.2809,0.0937,0.1353
njit_roberta,2021/06/23,njit,"Model that uses a simple text augmentation strategy for expanding training data. Then, we use a pre-trained RoBERTa model to generate text embeddings of tweets and classify them.",,,0.5721,0.2616,0.3055,0.8652,0.1599,0.1730,0.1728,0.2099
njit_label_prop,2021/07/08,njit,Label Propagation generation pipeline with priority scores using highest average score of information type labels.,,,0.5504,0.2072,0.2823,0.8841,0.2557,0.2719,0.1401,0.1373
njit_eda,2021/07/23,njit,NJIT RoBERTa run with Easy Data Augmentation (EDA) in place of synonym augmentation.,,,0.5655,0.2815🏆,0.3052,0.8778,0.1518,0.1730,0.1336,0.2220
