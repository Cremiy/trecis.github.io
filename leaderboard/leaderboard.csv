Run,date,team,description,paper,code,nDCG@100,Info-Type F1 [Actionable],Info-Type F1 [All],Info-Type Accuracy,Priority F1 [Actionable],Priority F1 [All],Priority R [Actionable],Priority R [All] 
0rule_run,2021/06/01,baseline,Simple baseline that ranks all tweets as the most common relevant info-type (Advice) and low priority.,,,0.3105,0.0000,0.0056,0.8673,0.0465,0.1360,0.0000,0.0000
random_run,2021/06/01,baseline,Simple baseline that assigns random labels and priorities to each tweet.,,,0.3287,0.0577,0.1408,0.5001,0.2136,0.2326,0.0153,0.0178
njit_bert,2021/06/01,njit,"Model that uses a simple text augmentation strategy for expanding training data. Then, we use a pre-trained BERT model to generate text embeddings of tweets and classify them.",,,0.4745,0.2346,0.2640,0.8181,0.1791,0.1641,0.0797,0.2353
njit_roberta,2021/06/01,njit,"Model that uses a simple text augmentation strategy for expanding training data. Then, we use a pre-trained RoBERTa model to generate text embeddings of tweets and classify them",,,0.4933,0.3138,0.3341,0.8644,0.2203,0.2035,0.2204,0.2640
2021a.sub.01,2021/06/01,anon.01,"INFO-roBERTa-base+2xTransformer+MultitaskEvent_type, 280, hashtags",,,0.4925,0.2857,0.3065,0.8917,0.2377,0.2742,0.2139,0.2313
2021a.sub.02,2021/06/01,anon.01,"PRIORITY-SVM-mixed INFO-roBERTa-base+2xTransformer+event_type+event_title, 280, combined",,,0.3019,0.2679,0.3240,0.8905,0.0655,0.1276,0.0229,-0.0024
2021a.sub.03,2021/06/01,anon.01,"PRIORITY-LR5 INFO-roBERTa-base+2xTransformer+event_type+event_title, 256, hashtags, combined",,,0.3678,0.1758,0.2923,0.8931,0.2042,0.2401,0.1001,0.0908
2021a.sub.04,2021/06/01,anon.01,"INFO-roBERTa-base+2xTransformer, 280 PRIORITY-roBERTa-base+2xTransformer+event_type+event_title, 280, hashtags, combined",,,0.5410,0.2996,0.3280,0.8870,0.2437,0.2592,0.2603,0.2425
2021a.sub.05,2021/06/01,anon.02,We used ktrain package to train our system,,,0.4285,0.1861,0.3108,0.8565,0.0864,0.1609,0.0231,0.1099
2021a.sub.06,2021/06/01,anon.03,Multi task training using deberta-base without eda augmentation.,,,0.5564,0.3057,0.3369,0.8841,0.2612,0.2966,0.2107,0.2967
2021a.sub.07,2021/06/01,anon.03,Multi task training using deberta-base with eda augmentation.,,,0.5579,0.2845,0.3393,0.8828,0.2410,0.2845,0.1646,0.2906
2021a.sub.08,2021/06/01,anon.03,multi-task learning using deberta-large without eda augmentation,,,0.5441,0.3287,0.3464,0.8858,0.2242,0.2970,0.1977,0.2648
2021a.sub.09,2021/06/01,anon.03,"multi-task learning ensembles of run1, 2, and 3",,,0.5525,0.0224,0.0615,0.7682,0.2517,0.2825,0.2159,0.2982
2021a.sub.10,2021/06/01,anon.04,"25 binary classifier + regression head on BERT base, using weighting schemes for balancing, each task using optimal tuned parameters from pool.",,,0.3561,0.0920,0.2203,0.8841,0.0575,0.1139,0.0514,0.0508
2021a.sub.11,2021/06/01,anon.04,"25 binary classifier + regression head on BERT base, using weighting schemes for balancing, each task using optimal tuned parameters from pool. Actionable tasks utilise transfer learning techniques based on label co-occurrence to alleviate data scarcity.",,,0.3551,0.1884,0.3140,0.8847,0.0575,0.1139,0.0514,0.0508
2021a.sub.12,2021/06/01,anon.04,"25 binary classifier + regression head on BERT base, using weighting schemes for balancing, each task using optimal tuned parameters from pool. All tasks utilise transfer learning techniques based on label co-occurrence to alleviate data scarcity.",,,0.3552,0.1884,0.3090,0.8857,0.0575,0.1139,0.0514,0.0508
njit_semi_sup,2021/06/27,njit,"Model uses GPT2 to generate tweets and semi-supervision to label them, using which we train a new transformer model.",,,0.4928,0.2597,0.3296,0.8897,0.2203,0.2035,0.2204,0.2640
ucdcs-mtl.ens,2021/06/29,ucd-cs,"The ensemble run of multi-task learning with Deberta and eda augmentation, i.e., ucdcs-run4 at 2021a (alignment issue fixed)",https://lill.is/site/pubs/Wang2021b.html,https://github.com/wangcongcong123/crisis-mtl,0.5539,0.3311üèÜ,0.3555üèÜ,0.8620,0.2517,0.2825,0.2159,0.2982üèÜ
njit_label_prop,2021/07/01,njit,"Model uses GPT2 to generate tweets and label propagation to label them, using which we train a new transformer model.",,,0.4928,0.2409,0.3165,0.8874,0.2203,0.2035,0.2204,0.2640
