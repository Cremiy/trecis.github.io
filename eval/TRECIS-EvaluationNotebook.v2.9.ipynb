{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC IS 2021a Evaluation Script\n",
    "# Configured for 2020-B Events\n",
    "# Used to evaluate TREC-IS runs\n",
    "# --------------------------------------------------\n",
    "version = 2.9 # Notebook Version Number\n",
    "edition = \"2021a\"\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Information\n",
    "\n",
    "# Do we try and normalize the run priority scores?\n",
    "enablePriorityNorm = True\n",
    "\n",
    "# Score threshold\n",
    "enableCategoryNorm = True\n",
    "defaultScoreThreshold = 0.5\n",
    "\n",
    "taskCategories = [\n",
    "    \"CallToAction-Donations\",\n",
    "    \"CallToAction-MovePeople\",\n",
    "    \"CallToAction-Volunteer\",\n",
    "    \"Other-Advice\",\n",
    "    \"Other-ContextualInformation\",\n",
    "    \"Other-Discussion\",\n",
    "    \"Other-Irrelevant\",\n",
    "    \"Other-Sentiment\",\n",
    "    \"Report-CleanUp\",\n",
    "    \"Report-EmergingThreats\",\n",
    "    \"Report-Factoid\",\n",
    "    \"Report-FirstPartyObservation\",\n",
    "    \"Report-Hashtags\",\n",
    "    \"Report-Location\",\n",
    "    \"Report-MultimediaShare\",\n",
    "    \"Report-News\",\n",
    "    \"Report-NewSubEvent\",\n",
    "    \"Report-Official\",\n",
    "    \"Report-OriginalEvent\",\n",
    "    \"Report-ServiceAvailable\",\n",
    "    \"Report-ThirdPartyObservation\",\n",
    "    \"Report-Weather\",\n",
    "    \"Request-GoodsServices\",\n",
    "    \"Request-InformationWanted\",\n",
    "    \"Request-SearchAndRescue\",\n",
    "]\n",
    "\n",
    "# What we consider to be highly important categories of information\n",
    "highImportCategories = [\n",
    "    \"Request-GoodsServices\",\n",
    "    \"Request-SearchAndRescue\",\n",
    "    \"CallToAction-MovePeople\",\n",
    "    \"Report-EmergingThreats\",\n",
    "    \"Report-NewSubEvent\",\n",
    "    \"Report-ServiceAvailable\"\n",
    "]\n",
    "\n",
    "highImportCategoriesShort = [\n",
    "    \"GoodsServices\",\n",
    "    \"SearchAndRescue\",\n",
    "    \"MovePeople\",\n",
    "    \"EmergingThreats\",\n",
    "    \"NewSubEvent\",\n",
    "    \"ServiceAvailable\"\n",
    "]\n",
    "\n",
    "# Priority map\n",
    "priorityScoreMap = {\n",
    "    \"Critical\": 1.0,\n",
    "    \"High\": 0.75,\n",
    "    \"Medium\": 0.5,\n",
    "    \"Low\": 0.25,\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "var_lambda = 0.75 # weight to place on actionable information categories in comparison to non actionable categoriee\n",
    "var_alpha = 0.3 # Flat gain for providing a correct alert, regardless of the categories selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events with no data, so we should skip them\n",
    "skipEvents = [\n",
    "\"2015_09_28_hurricane_joaquin.2015\",\n",
    "\"2017_03_23_cyclone_debbie.2017\",\n",
    "\"2018_02_24_anticyclone_hartmut.2018\",\n",
    "\"2018_07_13_ferguson_wildfire.2018\",\n",
    "\"2018_07_23_cranston_wildfire.2018\",\n",
    "\"2018_07_23_klamathon_wildfire.2018\",\n",
    "\"2018_08_05_holy_wildfire.2018\",\n",
    "\"2018_09_07_hurricane_florence.2018\",\n",
    "\"2018_10_07_hurricane_michael.2018\",\n",
    "\"2018_11_07_Woolsey_wildfire.2018\",\n",
    "\"2018_pittsburgh_synagogue_shooting\",\n",
    "\"2019_03_01_alberta_wildfire.2019.v2\",\n",
    "\"2019_08_25_hurricane_dorian.2019\",\n",
    "\"2019_09_17_tropicalstorm_imelda.2019\",\n",
    "\"2019_10_10_saddleridge_wildfire.2019\",\n",
    "\"2019_karnataka_floods\",\n",
    "\"2019_spring_floods_in_ontario_quebec_and_new_brunswick\",\n",
    "\"2019_townsville_flood\",\n",
    "\"2020_01_28_bar_shooting_nc.2020\",\n",
    "\"2020_02_07_rutherford_tn_floods.2020\",\n",
    "\"2020_02_10_mideast_tornadoes.day1_mississipi.2020\",\n",
    "\"2020_02_10_mideast_tornadoes.day3_md.2019\",\n",
    "\"2020_05_26_edenville_dam_failure.2020.corrected\",\n",
    "\"2020_08_27_hurricane_laura.2020\",\n",
    "\"2020_09_11_hurricane_sally.2020\",\n",
    "\"2020_afghanistan_flood\",\n",
    "\"2020_hpakant_jade_mine_disaster\",\n",
    "\"2020_kerala_floods\",\n",
    "\"2020_tornado_outbreak_of_march\",\n",
    "\"brooklynblockparty_shooting.2019\",\n",
    "\"indonesia_earthquake.2019\",\n",
    "\"T2020_02_03_texas_university_shooting.2020\",\n",
    "\"tornado_outbreak_of_november_30_december_2018\",\n",
    "\"UNASSIGNED\",\n",
    "    \n",
    "\"2020_01_27_houston_explosion.2020\", # 2021a.pt1\n",
    "\"2016_puttingal_temple\", # 2021a.pt1\n",
    "\"2017_12_04_thomas_wildfire.2017\", # 2021a.pt1\n",
    "\"2017_12_07_lilac_wildfire.2017\", # 2021a.pt1\n",
    "\"2018_maryland_flood\", # 2021a.pt1\n",
    "\"2019_saugus_high_school_shooting\", # 2021a.pt1\n",
    "\"2020_easter_tornado_outbreak\", # 2021a.pt1\n",
    "    \n",
    "# \"2019_10_25_kincade_wildfire.2019\",  # 2021a.pt2\n",
    "# \"2019_durham_gas_explosion\",  # 2021a.pt2\n",
    "# \"2020_02_10_mideast_tornadoes.day2_al.2020\",  # 2021a.pt2\n",
    "# \"2020_05_06_tn_derecho.2020\",  # 2021a.pt2\n",
    "# \"2020_tornado_outbreak_of_april\",  # 2021a.pt2\n",
    "# \"2020_visakhapatnam_gas_leak\",  # 2021a.pt2\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "runFile = None\n",
    "for f in glob.glob(\"*.gz\"):\n",
    "    runFile = f\n",
    "\n",
    "print(\"Run File:\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runName = None\n",
    "\n",
    "with gzip.open(runFile, \"r\") as inRunFile:\n",
    "    for line in inRunFile:\n",
    "        line = line.decode(\"utf8\")\n",
    "#         runName = line.rpartition(\"\\t\")[2].strip()\n",
    "        runName = json.loads(line)[\"runtag\"]\n",
    "        break\n",
    "\n",
    "print(\"Run Name:\", runName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we try and normalize the run priority scores?\n",
    "enablePriorityNorm = False\n",
    "\n",
    "dataDir = \"../../data/2021a\"\n",
    "\n",
    "# The location of the topics file\n",
    "topicsFile = \"%s/2021a.topics\" % dataDir\n",
    "\n",
    "# The location of the ground truth data against which to compare the run\n",
    "classificationLabelFiles = [\n",
    "#     \"%s/TRECIS-2021A-crisis.labels.prelim.json\" % dataDir,\n",
    "    \"%s/TRECIS-2021A-crisis.labels.prelim.pt2.json\" % dataDir,\n",
    "]\n",
    "\n",
    "# The location of the ontology file\n",
    "ontologyFile = \"%s/TRECIS-2021A-ITypes.json\" % dataDir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicArray = []\n",
    "\n",
    "with open(topicsFile, \"r\") as inTopicsFile:\n",
    "    \n",
    "    topicNum = None\n",
    "    topicDataset = None\n",
    "    \n",
    "    for line_ in inTopicsFile:\n",
    "        line = line_.strip()\n",
    "        \n",
    "        if line == \"</top>\":\n",
    "            if topicDataset in skipEvents:\n",
    "                continue\n",
    "            topicArray.append((topicDataset, topicNum))\n",
    "            \n",
    "        if line.startswith(\"<num>\"):\n",
    "            topicNum = line.partition(\"<num>\")[2].partition(\"</num>\")[0]\n",
    "            \n",
    "        if line.startswith(\"<dataset>\"):\n",
    "            topicDataset = line.partition(\"<dataset>\")[2].partition(\"</dataset>\")[0]\n",
    "            \n",
    "for row in topicArray:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Static data for the 2021 edition\n",
    "# --------------------------------------------------\n",
    "# Identifiers for the test events\n",
    "eventidTopicidMap = dict(topicArray)\n",
    "eventIdentifiers = list(eventidTopicidMap.keys())\n",
    "\n",
    "resultsFile = open(runName+\".results.v\"+str(version)+\".\"+edition+\".overall.txt\",\"w+\")\n",
    "resultsFile.write(\"TREC-IS \"+edition+\" Notebook Evaluator v\"+str(version)+\"\\n\")\n",
    "resultsFile.write(\"Run: \"+runName+\" (\"+runFile+\")\"+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")\n",
    "\n",
    "perTopicFile = open(runName+\".results.v\"+str(version)+\".\"+edition+\".pertopic.txt\",\"w+\")\n",
    "perTopicFile.write(\"TREC-IS \"+edition+\" Notebook Evaluator v\"+str(version)+\"\\n\")\n",
    "perTopicFile.write(\"Run: \"+runName+\" (\"+runFile+\")\"+\"\\n\")\n",
    "perTopicFile.write(\"\"+\"\\n\")\n",
    "\n",
    "perEventFile = open(runName+\".results.v\"+str(version)+\".\"+edition+\".perevent.txt\",\"w+\")\n",
    "perEventFile.write(\"TREC-IS \"+edition+\" Notebook Evaluator v\"+str(version)+\"\\n\")\n",
    "perEventFile.write(\"Run: \"+runName+\" (\"+runFile+\")\"+\"\\n\")\n",
    "perEventFile.write(\"\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Processing Starts Here\n",
    "# --------------------------------------------------\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 1: Load the ground truth dataset \n",
    "# --------------------------------------------------\n",
    "\n",
    "groundtruthJSON = []\n",
    "for groundtruthFile in classificationLabelFiles:\n",
    "    print(\"Reading \"+groundtruthFile)\n",
    "    with open(groundtruthFile, encoding='iso-8859-1') as groundtruthJSONFile:    \n",
    "        groundtruthJSON.append(json.load(groundtruthJSONFile))\n",
    "#pprint(groundtruthJSON[\"events\"])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 2: Load run file \n",
    "# --------------------------------------------------\n",
    "with gzip.open(runFile, \"r\") as openRunFile:\n",
    "#     runContents = [line.decode(\"utf8\") for line in openRunFile.readlines()] # lines not yet decoded\n",
    "    runContents = [json.loads(line.decode(\"utf8\")) for line in openRunFile.readlines()] # lines not yet decoded\n",
    "#pprint(runContents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Stage 3: Load the categories \n",
    "# --------------------------------------------------\n",
    "with open(ontologyFile, encoding='utf-8') as ontologyJSONFile:    \n",
    "    ontologyJSON = json.load(ontologyJSONFile)\n",
    "\n",
    "informationTypes2Index = {} # category -> numerical index\n",
    "informationTypesShort2Index = {} # category short form (e.g. Report-EmergingThreats vs. EmergingThreats) -> numerical index\n",
    "\n",
    "for informationTypeJSON in ontologyJSON[\"informationTypes\"]:\n",
    "    informationTypeId = informationTypeJSON[\"id\"]\n",
    "    \n",
    "    informationTypeIndex = taskCategories.index(informationTypeId)\n",
    "    informationTypes2Index[informationTypeId] = informationTypeIndex\n",
    "    informationTypesShort2Index[informationTypeId.split(\"-\")[1]] = informationTypeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Stage 4: Produce ground truth maps between tweetIds and categories\n",
    "# -----------------------------------------------------------\n",
    "# Notes: Ground truth is used as a base, if a run includes tweets\n",
    "#        not in the ground truth they will be ignored\n",
    "# Assumptions: A tweet will not be returned for multiple events\n",
    "\n",
    "tweetId2TRECInfoCategories = {} # tweet id -> Array of categories selected by assessors\n",
    "tweetId2TRECHighImportInfoCategories = {} # tweet id -> Array of categories selected by assessors\n",
    "tweetId2TRECLowImportInfoCategories = {} # tweet id -> Array of categories selected by assessors\n",
    "tweetId2TRECPriorityCategory = {} # tweet id -> priority label (Critical,High,Medium,Low)\n",
    "index2TweetId = {} # ordered tweets\n",
    "event2tweetIds = {} # event -> tweet ids for tweets within that event\n",
    "countHighCriticalImport = 0\n",
    "countLowMediumImport = 0\n",
    "tweetsSeen = []\n",
    "\n",
    "\n",
    "invertedPriorityScoreMap = {\n",
    "    v:k for k,v in priorityScoreMap.items()\n",
    "}\n",
    "\n",
    "tweetIndex = 0\n",
    "for groundtruth in groundtruthJSON:\n",
    "    for eventJSON in groundtruth[\"events\"]:\n",
    "        eventid = eventJSON[\"eventid\"]\n",
    "        print(eventid)\n",
    "        \n",
    "        if eventid in skipEvents:\n",
    "            continue\n",
    "        \n",
    "        if not event2tweetIds.get(eventid):\n",
    "            event2tweetIds[eventid] = []\n",
    "        \n",
    "        if any(eventid in s for s in eventIdentifiers):\n",
    "            # iterate over tweets in the event\n",
    "            for tweetJSON in eventJSON[\"tweets\"]:\n",
    "                tweetid = tweetJSON[\"postID\"]\n",
    "                categories = tweetJSON[\"postCategories\"]\n",
    "                priority = tweetJSON[\"postPriority\"]\n",
    "                \n",
    "                if priority == \"High\" or priority == \"Critical\":\n",
    "                    countHighCriticalImport = countHighCriticalImport + 1\n",
    "                \n",
    "                if priority == \"Low\" or priority == \"Medium\":\n",
    "                    countLowMediumImport = countLowMediumImport + 1\n",
    "                \n",
    "                # check categories for name issues and correct if possible\n",
    "                cleanedCategories = []\n",
    "                highImportCats = []\n",
    "                lowImportCats = []\n",
    "                for categoryId in categories:\n",
    "                    if not any(categoryId in s for s in informationTypesShort2Index.keys()):\n",
    "#                         print(\"Found unknown category in ground truth \"+categoryId+\", ignoring...\")\n",
    "                        pass\n",
    "                    else:\n",
    "                        cleanedCategories.append(categoryId)\n",
    "                        if any(categoryId in s for s in highImportCategoriesShort):\n",
    "                            highImportCats.append(categoryId)\n",
    "                        else:\n",
    "                            lowImportCats.append(categoryId)\n",
    "    \n",
    "                if tweetid not in tweetsSeen:\n",
    "                    event2tweetIds[eventid].append(tweetid)\n",
    "                    tweetId2TRECInfoCategories[tweetid] = cleanedCategories\n",
    "                    tweetId2TRECHighImportInfoCategories[tweetid] = highImportCats\n",
    "                    tweetId2TRECLowImportInfoCategories[tweetid] = lowImportCats\n",
    "                    tweetId2TRECPriorityCategory[tweetid] = priority\n",
    "                    index2TweetId[tweetIndex] = tweetid;\n",
    "                    tweetIndex = tweetIndex + 1\n",
    "                    tweetsSeen.append(tweetid)\n",
    "\n",
    "                else:\n",
    "                    tweetId2TRECInfoCategories[tweetid] = list(set(\n",
    "                        cleanedCategories + tweetId2TRECInfoCategories[tweetid]\n",
    "                    ))\n",
    "                    \n",
    "                    prePriorityScore = priorityScoreMap[tweetId2TRECPriorityCategory[tweetid]]\n",
    "                    thisPriorityScore = priorityScoreMap[priority]\n",
    "                    \n",
    "                    tweetId2TRECPriorityCategory[tweetid] = invertedPriorityScoreMap[\n",
    "                        max(prePriorityScore, thisPriorityScore)\n",
    "                    ]\n",
    "\n",
    "                \n",
    "        else:\n",
    "            print(\"WARN: Found ground truth data for event not in the topic set \"+eventid+\", ignoring...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Stage 5: Produce run predicted maps between tweetIds and categories\n",
    "# -----------------------------------------------------------\n",
    "tweetId2RunInfoCategories = {} # tweet id -> predicted category by participant system\n",
    "tweetId2RunHighImportInfoCategories = {} # tweet id -> predicted category by participant system\n",
    "tweetId2RunLowImportInfoCategories = {} # tweet id -> predicted category by participant system\n",
    "tweetId2RunInfoCategoriesProb = {} # tweet id -> predicted category probability by participant system\n",
    "tweetId2RunInfoCategoriesProbNorm = {} # tweet id -> predicted category probability by participant system\n",
    "tweetId2RunPriorityScore = {} # tweet id -> importance score from participant system\n",
    "tweetId2RunPriorityCategory = {} # tweet id -> importance category (Critical, High, Medium Low)\n",
    "tweetId2RunPriorityScoreNorm = {} # tweet id -> importance score from participant system\n",
    "event2TweetIdRank = {} # event -> (rank,tweetid)\n",
    "\n",
    "maxPrediction = -999999\n",
    "minPrediction = 999999\n",
    "maxCategory = -999999\n",
    "minCategory = 999999\n",
    "\n",
    "for predictionParts in runContents:\n",
    "    \n",
    "    #print(runLine)\n",
    "    if (len(predictionParts)<6 ):\n",
    "        print(runLine)\n",
    "        continue\n",
    "    else:\n",
    "        eventId = predictionParts[\"topic\"]\n",
    "        \n",
    "        if eventId in skipEvents:\n",
    "            continue\n",
    "        \n",
    "        tweetId = predictionParts[\"tweet_id\"]\n",
    "        rank = 0\n",
    "        #print(predictionParts[5])\n",
    "\n",
    "        category_scores = predictionParts[\"info_type_scores\"]\n",
    "        category_labels = predictionParts[\"info_type_labels\"]\n",
    "\n",
    "        priority = float(predictionParts[\"priority\"])\n",
    "        \n",
    "        if priority > maxPrediction:\n",
    "            maxPrediction = priority\n",
    "        if priority < minPrediction:\n",
    "            minPrediction = priority\n",
    "        \n",
    "        cleanedCategories = []\n",
    "        cleanedCategoriesProbs = []\n",
    "        highImportCats = []\n",
    "        lowImportCats = []\n",
    "        \n",
    "        # Handle category flags\n",
    "        for catIndex, categoryLabel in enumerate(category_labels):\n",
    "            # check if we have a binary flag for this label\n",
    "            if categoryLabel == 0:\n",
    "                # False flag, so skip\n",
    "                continue\n",
    "                \n",
    "            categoryId = taskCategories[catIndex]\n",
    "            \n",
    "            if not any(categoryId in s for s in informationTypes2Index.keys()):\n",
    "                print(\"Found unknown category in run \"+categoryId+\", ignoring...\")\n",
    "            else:\n",
    "                cleanedCategories.append(categoryId)\n",
    "                if any(categoryId in s for s in highImportCategories):\n",
    "                    highImportCats.append(categoryId)\n",
    "                else:\n",
    "                    lowImportCats.append(categoryId)\n",
    "                    \n",
    "        # Process category probabilities\n",
    "        for categoryProbability in category_scores:\n",
    "            \n",
    "            if categoryProbability > maxCategory:\n",
    "                maxCategory = categoryProbability\n",
    "            if categoryProbability < minCategory:\n",
    "                minCategory = categoryProbability\n",
    "            \n",
    "            cleanedCategoriesProbs.append(categoryProbability)\n",
    "                \n",
    "        tweetId2RunHighImportInfoCategories[tweetId] = highImportCats\n",
    "        tweetId2RunLowImportInfoCategories[tweetId] = lowImportCats\n",
    "        tweetId2RunInfoCategories[tweetId] = cleanedCategories\n",
    "        tweetId2RunInfoCategoriesProb[tweetId] = cleanedCategoriesProbs\n",
    "        tweetId2RunPriorityScore[tweetId] = priority\n",
    "        \n",
    "        if priority > priorityScoreMap[\"High\"]:\n",
    "            tweetId2RunPriorityCategory[tweetId] = \"Critical\"\n",
    "        elif priority > priorityScoreMap[\"Medium\"]:\n",
    "            tweetId2RunPriorityCategory[tweetId] = \"High\"\n",
    "        elif priority > priorityScoreMap[\"Low\"]:\n",
    "            tweetId2RunPriorityCategory[tweetId] = \"Medium\"\n",
    "        else:\n",
    "            tweetId2RunPriorityCategory[tweetId] = \"Low\"\n",
    "        \n",
    "        if not event2TweetIdRank.get(eventId):\n",
    "            event2TweetIdRank[eventId] = []\n",
    "        rankTuple = (tweetId,rank)\n",
    "        event2TweetIdRank.get(eventId).append(rankTuple)\n",
    "\n",
    "\n",
    "for eventId in event2TweetIdRank.keys():\n",
    "    tweetsSorted = sorted(event2TweetIdRank.get(eventId), key=lambda tup: tup[1])\n",
    "    event2TweetIdRank[eventId] = tweetsSorted\n",
    "    \n",
    "for i in range(len(index2TweetId)):\n",
    "    tweetId = index2TweetId[i]\n",
    "    if tweetId2RunPriorityScore.get(tweetId):\n",
    "        \n",
    "        if enablePriorityNorm:\n",
    "            if (minPrediction-minPrediction) == 0.0:\n",
    "                tweetId2RunPriorityScoreNorm[tweetId] = 0.0\n",
    "            else:\n",
    "                tweetId2RunPriorityScoreNorm[tweetId] = (tweetId2RunPriorityScore.get(tweetId)-minPrediction)/(maxPrediction-minPrediction)\n",
    "        else:\n",
    "            tweetId2RunPriorityScoreNorm[tweetId] = tweetId2RunPriorityScore.get(tweetId)\n",
    "    else:\n",
    "        tweetId2RunPriorityScoreNorm[tweetId] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Stage 6: Create ground truth vectors per category\n",
    "# --------------------------------------------------\n",
    "\n",
    "category2GroundTruth = {} # category -> tweet vector with binary 1 vs all ground truth category labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    categoryVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "        #pprint(categories)\n",
    "        if any(categoryIdShort in s for s in categories):\n",
    "            categoryVector.append(1)\n",
    "        else:\n",
    "            categoryVector.append(0)\n",
    "    category2GroundTruth[categoryId] = categoryVector\n",
    "            \n",
    "#pprint(category2GroundTruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Stage 7: Create run vectors per category \n",
    "# --------------------------------------------------\n",
    "# Assumptions: If run misses a tweet, we assume it has\n",
    "#              no categories\n",
    "category2Predicted = {} # category -> tweet vector with binary 1 vs all predicted by system labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    categoryVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        \n",
    "        if tweetId2RunInfoCategories.get(tweetId):\n",
    "            categories = tweetId2RunInfoCategories.get(tweetId)\n",
    "            if any(categoryIdShort in s for s in categories):\n",
    "                categoryVector.append(1)\n",
    "            else:\n",
    "                categoryVector.append(0)\n",
    "        else:\n",
    "            categoryVector.append(0)\n",
    "\n",
    "    category2Predicted[categoryId] = categoryVector\n",
    "\n",
    "#pprint(category2Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Stage 8: Make event category vectors \n",
    "# --------------------------------------------------\n",
    "\n",
    "event2groundtruth = {} # event -> category -> tweet vector with binary 1 vs all ground truth category labels\n",
    "for eventId in eventIdentifiers:\n",
    "    eventCategories = {}\n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        categoryIdShort = categoryId.split(\"-\")[1]\n",
    "        categoryVector = []\n",
    "#         print(eventId)\n",
    "        for tweetId in event2tweetIds.get(eventId):\n",
    "#             print(tweetId)\n",
    "            categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "            if any(categoryIdShort in s for s in categories):\n",
    "                categoryVector.append(1)\n",
    "            else:\n",
    "                categoryVector.append(0)\n",
    "            \n",
    "        eventCategories[categoryId] = categoryVector\n",
    "    event2groundtruth[eventId] = eventCategories\n",
    "    \n",
    "\n",
    "event2prediction = {} # event -> category -> tweet vector with binary 1 vs all predicted by system labels\n",
    "for eventId in eventIdentifiers:\n",
    "    print(eventId)\n",
    "    eventCategories = {}\n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        categoryIdShort = categoryId.split(\"-\")[1]\n",
    "        categoryVector = []\n",
    "#         print(tweetId)\n",
    "        for tweetId in event2tweetIds.get(eventId):\n",
    "            #print(tweetId)\n",
    "            categories = tweetId2RunInfoCategories.get(tweetId)\n",
    "            \n",
    "            if categories == None:\n",
    "                categories = json.loads(\"[]\")\n",
    "                tweetId2RunInfoCategories[tweetId] = categories\n",
    "            \n",
    "            if any(categoryId in s for s in categories):\n",
    "                categoryVector.append(1)\n",
    "            else:\n",
    "                categoryVector.append(0)\n",
    "            \n",
    "        eventCategories[categoryId] = categoryVector\n",
    "    event2prediction[eventId] = eventCategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Stage 9: Make priority classification vectors\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "category2GroundTruthPriority = {} # category -> tweet vector with binary 1 vs all ground truth priority labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    priorityVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "        if any(categoryIdShort in s for s in categories):\n",
    "            priority = tweetId2TRECPriorityCategory.get(tweetId)\n",
    "            priorityVector.append(priority)\n",
    "    category2GroundTruthPriority[categoryId] = priorityVector\n",
    "\n",
    "category2PredictedPriority = {} # category -> tweet vector with binary 1 vs all predicted by system labels\n",
    "category2PredictedPriorityScore = {} # Category -> tweet vector with priority scores\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    categoryVector = []\n",
    "    categoryScoreVector = []\n",
    "    \n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "        if any(categoryIdShort in s for s in categories):\n",
    "            if tweetId2RunPriorityCategory.get(tweetId):\n",
    "                priority = tweetId2RunPriorityCategory.get(tweetId)\n",
    "                priorityScore = tweetId2RunPriorityScore.get(tweetId)\n",
    "            \n",
    "                categoryVector.append(priority)\n",
    "                categoryScoreVector.append(priorityScore)\n",
    "            else:\n",
    "                categoryVector.append(\"Low\") # default to low priority\n",
    "                categoryScoreVector.append(0.25)\n",
    "\n",
    "    category2PredictedPriority[categoryId] = categoryVector\n",
    "    category2PredictedPriorityScore[categoryId] = categoryScoreVector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Disable Warnings (comment this out when debugging!)\n",
    "# --------------------------------------------------\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\") # ignore warnings about 0-score categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021A\n",
    "# Priority-Centric Discounted Cumulative Gain\n",
    "# --------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def calc_dcg(scores, at_k=100):\n",
    "    position = 1\n",
    "    accumulator = 0.0\n",
    "    for score in scores[:at_k]:\n",
    "\n",
    "        numerator = 2 ** score - 1\n",
    "        denom = np.log2(position + 1)\n",
    "\n",
    "        accumulator += numerator / denom\n",
    "        position += 1\n",
    "\n",
    "    return accumulator\n",
    "\n",
    "priority_map = {\n",
    "    \"Low\": 1,\n",
    "    \"Medium\": 2,\n",
    "    \"High\": 3,\n",
    "    \"Critical\": 4,\n",
    "}\n",
    "\n",
    "at_k = 100\n",
    "\n",
    "tweetId2TRECPriorityCategory_score = {\n",
    "    k:priority_map[v] for k,v in tweetId2TRECPriorityCategory.items()\n",
    "}\n",
    "tweetId2TRECPriorityCategory_scores_sorted = sorted(\n",
    "    tweetId2TRECPriorityCategory_score.values(),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "best_dcg_per_event = {}\n",
    "for event, rel_tweets in event2tweetIds.items():\n",
    "    print(event)\n",
    "    \n",
    "    tweetId2TRECPriorityCategory_scores_sorted = sorted(\n",
    "        [tweetId2TRECPriorityCategory_score[x] for x in rel_tweets],\n",
    "        reverse=True\n",
    "    )\n",
    "    ideal_dcg = calc_dcg(tweetId2TRECPriorityCategory_scores_sorted, at_k)\n",
    "    print(\"\\tBest DCG:\", ideal_dcg)\n",
    "    best_dcg_per_event[event] = ideal_dcg\n",
    "    \n",
    "print(\"Mean:\", np.mean(list(best_dcg_per_event.values())))\n",
    "print()\n",
    "\n",
    "# Code below calculates the DCG for a system's \n",
    "#  ranked priority tweets. We have to do some \n",
    "#  sampling here to break ties among tweets with\n",
    "#  the same priority scores.\n",
    "\n",
    "# Build a dataframe from the system's provided\n",
    "#  priority scores, so we can identify what the\n",
    "#  top-most priorities are and get a count of\n",
    "#  the number of tweets in each priority bin.\n",
    "priority_df = pd.DataFrame(\n",
    "    [(k, priority_map[v]) for k, v in tweetId2RunPriorityCategory.items()],\n",
    "    columns=[\"tweet_id\", \"priority\"]\n",
    ")\n",
    "\n",
    "# Build metrics for each event\n",
    "system_dcg_per_event = {}\n",
    "for event, rel_tweets in event2tweetIds.items():\n",
    "    print(\"Event:\", event)\n",
    "    local_priority_df = priority_df[priority_df[\"tweet_id\"].isin(set(rel_tweets))]\n",
    "    \n",
    "    unique_scores = local_priority_df[\"priority\"].value_counts()\n",
    "    \n",
    "    # Find the top priority scores that would be included\n",
    "    #  in the necessary at_k values.\n",
    "    total = 0\n",
    "    top_keys = []\n",
    "    candidates = {}\n",
    "    for top in sorted(unique_scores.index, reverse=True):\n",
    "\n",
    "        # We store this key, so we can go back and shuffle\n",
    "        #. tweets with this score.\n",
    "        top_keys.append(top)\n",
    "        local_restricted_df = local_priority_df[local_priority_df[\"priority\"] == top]\n",
    "        candidates[top] = list(local_restricted_df[\"tweet_id\"])\n",
    "\n",
    "        total += local_restricted_df.shape[0]\n",
    "\n",
    "        # Once we have enough samples, stop.\n",
    "        if ( total > at_k ):\n",
    "            break\n",
    "\n",
    "    # Now we generate distribution over the DCG for this\n",
    "    #  system and do this a number of times to remove\n",
    "    #  dependence on our selection of the top k tweets\n",
    "    random_dcgs = []\n",
    "    for i in range(100):\n",
    "\n",
    "        local_tweet_ids = []\n",
    "        for top in top_keys:\n",
    "            this_top_tweets = candidates[top][:]\n",
    "            np.random.shuffle(this_top_tweets)\n",
    "\n",
    "            needed = at_k - len(local_tweet_ids)\n",
    "            local_tweet_ids.extend(this_top_tweets[:needed])\n",
    "\n",
    "        local_scores = [tweetId2TRECPriorityCategory_score[x] for x in local_tweet_ids]\n",
    "\n",
    "        random_dcgs.append(calc_dcg(local_scores))\n",
    "\n",
    "    system_dcg = np.mean(random_dcgs)\n",
    "\n",
    "    system_ndcg_ = system_dcg / best_dcg_per_event[event]\n",
    "    print(\"\\tnDCG:\", system_ndcg_)\n",
    "    system_dcg_per_event[event] = system_ndcg_\n",
    "    \n",
    "print()\n",
    "system_ndcg_micro = np.mean(list(system_dcg_per_event.values()))\n",
    "print(\"System Event-Micro nDCG:\", system_ndcg_micro)\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: nDCG and Priority\"+\"\\n\")\n",
    "resultsFile.write(\"Overall performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> nDCG:\"+\"\\t\"+str(system_ndcg_micro)+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Type Categorization\n",
    "# Overall performance\n",
    "# --------------------------------------------------\n",
    "# Average performance over information types\n",
    "# Macro averaged (information types have equal weight)\n",
    "# Does not average across events (larger events have more impact)\n",
    "# Positive class is the target class\n",
    "# Precision, recall and F1 only consider the positive class\n",
    "# Accuracy is an overall metric\n",
    "# We report performance for all categories, high importance categories and low importance categories\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "avgPrecision = 0.0\n",
    "avgRecall = 0.0\n",
    "avgF1 = 0.0\n",
    "avgAccuracy = 0.0\n",
    "\n",
    "avgPrecisionHigh = 0.0\n",
    "avgRecallHigh = 0.0\n",
    "avgF1High = 0.0\n",
    "avgAccuracyHigh = 0.0\n",
    "\n",
    "avgPrecisionLow = 0.0\n",
    "avgRecallLow = 0.0\n",
    "avgF1Low = 0.0\n",
    "avgAccuracyLow = 0.0\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryPrecision = precision_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    categoryRecall = recall_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    categoryF1 = f1_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    categoryAccuracy = accuracy_score(category2GroundTruth[categoryId], category2Predicted[categoryId])\n",
    "    \n",
    "    avgPrecision = avgPrecision + categoryPrecision\n",
    "    avgRecall = avgRecall + categoryRecall\n",
    "    avgF1 = avgF1 + categoryF1\n",
    "    avgAccuracy = avgAccuracy + categoryAccuracy\n",
    "    \n",
    "    if any(categoryId in s for s in highImportCategories):\n",
    "        avgPrecisionHigh = avgPrecisionHigh + categoryPrecision\n",
    "        avgRecallHigh = avgRecallHigh + categoryRecall\n",
    "        avgF1High = avgF1High + categoryF1\n",
    "        avgAccuracyHigh = avgAccuracyHigh + categoryAccuracy\n",
    "    else:\n",
    "        avgPrecisionLow = avgPrecisionLow + categoryPrecision\n",
    "        avgRecallLow = avgRecallLow + categoryRecall\n",
    "        avgF1Low = avgF1Low + categoryF1\n",
    "        avgAccuracyLow = avgAccuracyLow + categoryAccuracy\n",
    "\n",
    "numInformationTypes = len(informationTypes2Index)\n",
    "numHighInformationTypes = len(highImportCategories)\n",
    "numLowInformationTypes = numInformationTypes - numHighInformationTypes\n",
    "        \n",
    "print(\"Information Type Precision (positive class, multi-type, macro): \"+str(avgPrecision/numInformationTypes))\n",
    "print(\"Information Type Recall (positive class, multi-type, macro): \"+str(avgRecall/numInformationTypes))\n",
    "print(\"Information Type F1 (positive class, multi-type, macro): \"+str(avgF1/numInformationTypes))\n",
    "print(\"Information Type Accuracy (overall, multi-type, macro): \"+str(avgAccuracy/numInformationTypes))\n",
    "\n",
    "print(\"High Importance Information Type Precision (positive class, multi-type, macro): \"+str(avgPrecisionHigh/numHighInformationTypes))\n",
    "print(\"High Importance Information Type Recall (positive class, multi-type, macro): \"+str(avgRecallHigh/numHighInformationTypes))\n",
    "print(\"High Importance Information Type F1 (positive class, multi-type, macro): \"+str(avgF1High/numHighInformationTypes))\n",
    "print(\"High Importance Information Type Accuracy (overall, multi-type, macro): \"+str(avgAccuracyHigh/numHighInformationTypes))\n",
    "\n",
    "print(\"Low Importance Information Type Precision (positive class, multi-type, macro): \"+str(avgPrecisionLow/numLowInformationTypes))\n",
    "print(\"Low Importance Information Type Recall (positive class, multi-type, macro): \"+str(avgRecallLow/numLowInformationTypes))\n",
    "print(\"Low Importance Information Type F1 (positive class, multi-type, macro): \"+str(avgF1Low/numLowInformationTypes))\n",
    "print(\"Low Importance Information Type Accuracy (overall, multi-type, macro): \"+str(avgAccuracyLow/numLowInformationTypes))\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: Information Type Categorization\"+\"\\n\")\n",
    "resultsFile.write(\"Overall performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> Information Type Precision (positive class, multi-type, macro):\"+\"\\t\"+str(avgPrecision/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Information Type Recall (positive class, multi-type, macro):\"+\"\\t\"+str(avgRecall/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Information Type F1 (positive class, multi-type, macro):\"+\"\\t\"+str(avgF1/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Information Type Accuracy (overall, multi-type, macro):\"+\"\\t\"+str(avgAccuracy/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type Precision (positive class, multi-type, macro):\"+\"\\t\"+str(avgPrecisionHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type Recall (positive class, multi-type, macro):\"+\"\\t\"+str(avgRecallHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type F1 (positive class, multi-type, macro):\"+\"\\t\"+str(avgF1High/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type Accuracy (overall, multi-type, macro):\"+\"\\t\"+str(avgAccuracyHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type Precision (positive class, multi-type, macro):\"+\"\\t\"+str(avgPrecisionLow/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type Recall (positive class, multi-type, macro):\"+\"\\t\"+str(avgRecallLow/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type F1 (positive class, multi-type, macro):\"+\"\\t\"+str(avgF1Low/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type Accuracy (overall, multi-type, macro):\"+\"\\t\"+str(avgAccuracyLow/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Type Categorization\n",
    "# Per Information Type Performance\n",
    "# --------------------------------------------------\n",
    "# Per Category Classification Performance with confusion matrices\n",
    "# Performance on the target class is what we care about here, \n",
    "# primaraly with respect to recall, as we want the user to \n",
    "# see all of the information for a given category. A small\n",
    "# amount of noise being added to the feed is an acceptable\n",
    "# cost for good recall.\n",
    "#\n",
    "# Does not average across events (larger events have more impact)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perTopicFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "perTopicFile.write(\"EVALUATON: Information Type Categorization (Multi-type)\"+\"\\n\")\n",
    "perTopicFile.write(\"Per Information Type Performance\"+\"\\n\")\n",
    "perTopicFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    target_names = ['Other Classes', categoryId]\n",
    "    try:\n",
    "        print(categoryId)\n",
    "        print(classification_report(category2GroundTruth[categoryId], category2Predicted[categoryId], target_names=target_names))\n",
    "\n",
    "\n",
    "        perTopicFile.write(categoryId+\"\\n\")\n",
    "        perTopicFile.write(classification_report(category2GroundTruth[categoryId], category2Predicted[categoryId], target_names=target_names)+\"\\n\")\n",
    "        perTopicFile.write(\"\"+\"\\n\")\n",
    "      \n",
    "    except ValueError:\n",
    "        print(\"Category \"+categoryId+\" score calculation failed, likely due the category not being used by the run\")\n",
    "perTopicFile.write(\"\"+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Type Categorization\n",
    "# Per Information Type F1 Graph\n",
    "# --------------------------------------------------\n",
    "# Per Category Classification Performance\n",
    "# F1 scores for each information type, graphed\n",
    "# Does not average across events (larger events have more impact)\n",
    "\n",
    "\n",
    "\n",
    "N = len(informationTypes2Index)\n",
    "ind = np.arange(N)\n",
    "\n",
    "scoresPerCategoryF1 = []\n",
    "categoryLabels = []\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    localF1Score = f1_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    print(categoryId, localF1Score)\n",
    "    scoresPerCategoryF1.append(localF1Score)\n",
    "    categoryLabels.append(categoryId)\n",
    "    \n",
    "width = 0.90       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, scoresPerCategoryF1, width)\n",
    "\n",
    "plt.ylabel('F1 Scores')\n",
    "plt.title('F1 Scores by Information Type')\n",
    "plt.xticks(ind, categoryLabels, rotation='vertical')\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Type Categorization\n",
    "# Per Event Performance\n",
    "# --------------------------------------------------\n",
    "# Categorization performance for each event\n",
    "# Precision, recall and F1 only consider the positive class\n",
    "# Accuracy is an overall metric\n",
    "# We report performance for all categories, high importance categories and low importance categories\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "perEventFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "perEventFile.write(\"EVALUATON: Information Type Categorization (Multi-type)\"+\"\\n\")\n",
    "perEventFile.write(\"Per Event Performance\"+\"\\n\")\n",
    "perEventFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "\n",
    "for eventId in eventIdentifiers:\n",
    "    tavgPrecision = 0.0\n",
    "    tavgRecall = 0.0\n",
    "    tavgF1 = 0.0\n",
    "    tavgAccuracy = 0.0\n",
    "\n",
    "    categoryCount = 0\n",
    "    \n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        if sum(event2groundtruth[eventId].get(categoryId)) == 0:\n",
    "            continue\n",
    "        \n",
    "        categoryPrecision = precision_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        categoryRecall = recall_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        categoryF1 = f1_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        categoryAccuracy = accuracy_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId))\n",
    "        \n",
    "        tavgPrecision = tavgPrecision + categoryPrecision\n",
    "        tavgRecall = tavgRecall + categoryRecall\n",
    "        tavgF1 = tavgF1 + categoryF1\n",
    "        tavgAccuracy = tavgAccuracy + categoryAccuracy\n",
    "        \n",
    "        categoryCount += 1\n",
    "    \n",
    "    if categoryCount == 0:\n",
    "        print(\"No categories for event:\", eventId)\n",
    "        continue\n",
    "    \n",
    "    print(eventId)\n",
    "    print(\"  Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecision/categoryCount))\n",
    "    print(\"  Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecall/categoryCount))\n",
    "    print(\"  Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1/categoryCount))\n",
    "    print(\"  Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracy/categoryCount))\n",
    "    print(\"\")\n",
    "    \n",
    "    perEventFile.write(eventId+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecision/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecall/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracy/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"\\n\")\n",
    "    \n",
    "perEventFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Type Categorization\n",
    "# Per Event F1 Graph\n",
    "# --------------------------------------------------\n",
    "# Multi-type (1 vs All): Tweets have multiple information types, aim: predict all of them\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "N = len(eventIdentifiers)\n",
    "ind = np.arange(N)\n",
    "\n",
    "scoresPerEventF1 = []\n",
    "for eventId in eventIdentifiers:\n",
    "    avgF1_ = 0.0\n",
    "    \n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        avgF1_ = avgF1_ + f1_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        \n",
    "    scoresPerEventF1.append(avgF1_/len(informationTypes2Index))\n",
    "    \n",
    "width = 0.90       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, scoresPerEventF1, width)\n",
    "\n",
    "plt.ylabel('F1 Scores')\n",
    "plt.title('F1 Category Scores by Event')\n",
    "plt.xticks(ind, eventIdentifiers, rotation='vertical')\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Priority Level\n",
    "# Overall Performance\n",
    "# --------------------------------------------------\n",
    "# How divergent is the system from the human priority labels?\n",
    "# F1 performance over information types, higher is better\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "priorityAvgf1 = 0.0;\n",
    "priorityAvgf1High = 0.0;\n",
    "priorityAvgf1Low = 0.0;\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    groundTruthPriorities = category2GroundTruthPriority[categoryId]\n",
    "    predictedPriorities = category2PredictedPriority[categoryId]\n",
    "\n",
    "    f1 = f1_score(groundTruthPriorities, predictedPriorities, average='macro')\n",
    "    priorityAvgf1 = priorityAvgf1 + f1;\n",
    "    \n",
    "    if any(categoryId in s for s in highImportCategories):\n",
    "        priorityAvgf1High = priorityAvgf1High + f1\n",
    "    else:\n",
    "        priorityAvgf1Low = priorityAvgf1Low + f1\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Priority Label Prediction (F1, macro): \"+str(priorityAvgf1/len(informationTypes2Index)))\n",
    "    \n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: Information Priority Level\"+\"\\n\")\n",
    "resultsFile.write(\"Overall Performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> Priority Label Prediction (F1, macro): \"+str(priorityAvgf1/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Priority Level\n",
    "# Overall Performance\n",
    "# --------------------------------------------------\n",
    "# How divergent is the system from the human priority labels?\n",
    "# Use Pearson correlation here to capture parallel increases\n",
    "\n",
    "priorityAvgCorr = 0.0\n",
    "priorityAvgCorrHigh = 0.0\n",
    "priorityAvgCorrLow = 0.0\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    if categoryId == \"Other-Irrelevant\":\n",
    "        continue\n",
    "        \n",
    "    groundTruthPriorities = [priorityScoreMap[x] for x in category2GroundTruthPriority[categoryId]]\n",
    "    predictedPriorities = category2PredictedPriorityScore[categoryId]\n",
    "\n",
    "    # Pathological case when no variation exists in the predictions needs to be handled\n",
    "    this_corr = 0.0\n",
    "    if np.mean(np.array(predictedPriorities) - np.mean(predictedPriorities)) != 0.0:\n",
    "        this_corr = np.corrcoef(groundTruthPriorities, predictedPriorities)[0,1]\n",
    "    priorityAvgCorr = priorityAvgCorr + this_corr\n",
    "    \n",
    "    if any(categoryId in s for s in highImportCategories):\n",
    "        priorityAvgCorrHigh = priorityAvgCorrHigh + this_corr\n",
    "    else:\n",
    "        priorityAvgCorrLow = priorityAvgCorrLow + this_corr\n",
    "    \n",
    "print(\"Priority Score Prediction (Pearson): \"+str(priorityAvgCorr/(len(informationTypes2Index)-1)))\n",
    "print(\"Priority Score Prediction, High (Pearson): \"+str(priorityAvgCorrHigh/numHighInformationTypes))\n",
    "print(\"Priority Score Prediction, Low (Pearson): \"+str(priorityAvgCorrLow/(numLowInformationTypes-1)))\n",
    "\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: Information Priority Score\"+\"\\n\")\n",
    "resultsFile.write(\"Correlational Performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> Priority Correlation (Pearson): \"+str(priorityAvgCorr/(len(informationTypes2Index)-1))+\"\\n\")\n",
    "resultsFile.write(\"> Priority Correlation, High (Pearson): \"+str(priorityAvgCorrHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Priority Correlation, Low (Pearson): \"+str(priorityAvgCorrLow/(numLowInformationTypes-1))+\"\\n\")\n",
    "resultsFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2021-A\n",
    "# Information Priority Level\n",
    "# Per Information Type Performance\n",
    "# --------------------------------------------------\n",
    "# F1 per information type (macro averaged), higher is better\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "N = len(informationTypes2Index)\n",
    "ind = np.arange(N)\n",
    "\n",
    "priorityCatF1Values = []\n",
    "categoryLabels = []\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    groundTruthPriorities = category2GroundTruthPriority[categoryId]\n",
    "    predictedPriorities = category2PredictedPriority[categoryId]\n",
    "    priorityCatF1 = f1_score(groundTruthPriorities, predictedPriorities, average='macro')\n",
    "    if (math.isnan(priorityCatF1)):\n",
    "        priorityCatF1 = 0.0\n",
    "    categoryLabels.append(categoryId)\n",
    "    priorityCatF1Values.append(priorityCatF1);\n",
    "    \n",
    "width = 0.90       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, priorityCatF1Values, width)\n",
    "\n",
    "plt.ylabel('Priorty Label Prediction F1 (higher is better)')\n",
    "plt.title('Priorty Label Prediction F1 Per Information Type')\n",
    "plt.xticks(ind, categoryLabels, rotation='vertical')\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultLine = None\n",
    "\n",
    "# Print the evaluation table row in latex\n",
    "print(\"Run & NDCG & CF1-H & CF1-A & CAcc & PErr-H & PErr-A & PCorr-H & PCorr-A \\\\\\\\\")\n",
    "\n",
    "resultLine = (str.format('{0:.4f}', system_ndcg_micro)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',avgF1High/numHighInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',avgF1/numInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',avgAccuracy/numInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',priorityAvgf1High/numHighInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',priorityAvgf1/len(informationTypes2Index))+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',priorityAvgCorrHigh/numHighInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',priorityAvgCorr/len(informationTypes2Index))+\n",
    "     \" \\\\\\\\\")\n",
    "\n",
    "print(runName+\" & \"+resultLine)\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"LATEX\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(runName+\" & \"+resultLine + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "resultsFile.close() \n",
    "perTopicFile.close()\n",
    "perEventFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
